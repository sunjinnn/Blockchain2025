{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOm2cIPu8yy4BTmHd51Rltp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sunjinnn/Blockchain2025/blob/main/25_10_28_%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1_%ED%9A%8C%EA%B7%80.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 로지스틱 회귀"
      ],
      "metadata": {
        "id": "mM4KCIWOeU7T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcaCzG9Dgo6u",
        "outputId": "3a56c81d-450b-4988-dab9-b4522e965f6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9327731092436975\n",
            "0.925\n",
            "['Perch' 'Smelt' 'Pike' 'Whitefish' 'Perch' 'Bream' 'Smelt' 'Roach'\n",
            " 'Perch' 'Pike' 'Bream' 'Whitefish' 'Bream' 'Parkki' 'Bream' 'Bream'\n",
            " 'Perch' 'Perch' 'Perch' 'Bream' 'Smelt' 'Bream' 'Bream' 'Bream' 'Bream'\n",
            " 'Perch' 'Perch' 'Whitefish' 'Smelt' 'Smelt' 'Pike' 'Perch' 'Perch' 'Pike'\n",
            " 'Bream' 'Perch' 'Roach' 'Roach' 'Parkki' 'Perch']\n",
            "['Perch' 'Smelt' 'Pike' 'Roach' 'Perch' 'Bream' 'Smelt' 'Roach' 'Perch'\n",
            " 'Pike' 'Bream' 'Perch' 'Bream' 'Parkki' 'Bream' 'Bream' 'Perch' 'Perch'\n",
            " 'Perch' 'Bream' 'Smelt' 'Bream' 'Bream' 'Bream' 'Bream' 'Perch' 'Perch'\n",
            " 'Roach' 'Smelt' 'Smelt' 'Pike' 'Perch' 'Perch' 'Pike' 'Bream' 'Perch'\n",
            " 'Roach' 'Roach' 'Parkki' 'Perch']\n",
            "['Bream' 'Parkki' 'Perch' 'Pike' 'Roach' 'Smelt' 'Whitefish']\n",
            "[[0.  0.  0.8 0.  0.1 0.  0. ]\n",
            " [0.  0.  0.  0.  0.  0.9 0. ]\n",
            " [0.  0.  0.  0.9 0.  0.  0. ]\n",
            " [0.  0.  0.3 0.  0.6 0.  0.1]\n",
            " [0.  0.  0.9 0.  0.1 0.  0. ]\n",
            " [1.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.1 0.  0.  0.9 0. ]\n",
            " [0.  0.  0.3 0.  0.6 0.  0. ]\n",
            " [0.  0.  0.8 0.  0.2 0.  0. ]\n",
            " [0.  0.  0.  1.  0.  0.  0. ]\n",
            " [1.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.8 0.  0.  0.  0.1]\n",
            " [1.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.9 0.  0.  0.1 0.  0. ]\n",
            " [1.  0.  0.  0.  0.  0.  0. ]\n",
            " [1.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.9 0.  0.  0.  0.1]\n",
            " [0.  0.  0.8 0.  0.2 0.  0. ]\n",
            " [0.  0.  0.7 0.  0.3 0.  0. ]\n",
            " [1.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  0.9 0. ]\n",
            " [1.  0.  0.  0.  0.  0.  0. ]\n",
            " [1.  0.  0.  0.  0.  0.  0. ]\n",
            " [1.  0.  0.  0.  0.  0.  0. ]\n",
            " [1.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  1.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.8 0.  0.2 0.  0. ]\n",
            " [0.  0.  0.3 0.  0.6 0.  0.1]\n",
            " [0.  0.  0.  0.  0.  1.  0. ]\n",
            " [0.  0.  0.  0.  0.  1.  0. ]\n",
            " [0.  0.  0.  1.  0.  0.  0. ]\n",
            " [0.  0.  0.8 0.  0.1 0.  0. ]\n",
            " [0.  0.  0.7 0.  0.2 0.  0. ]\n",
            " [0.  0.  0.  1.  0.  0.  0. ]\n",
            " [0.7 0.2 0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.6 0.  0.3 0.  0. ]\n",
            " [0.  0.  0.2 0.  0.7 0.  0. ]\n",
            " [0.  0.  0.3 0.  0.7 0.  0. ]\n",
            " [0.  0.9 0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.7 0.  0.  0.  0.3]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "fish = pd.read_csv('http://bit.ly/fish_csv_data')\n",
        "fish_input = fish[['Weight','Length','Diagonal','Height','Width']].to_numpy()\n",
        "fish_target = fish['Species'].to_numpy()\n",
        "\n",
        "train_input, test_input, train_target, test_target = train_test_split(fish_input, fish_target, random_state=42)\n",
        "\n",
        "ss = StandardScaler()\n",
        "ss.fit(train_input)\n",
        "train_scaled = ss.transform(train_input)\n",
        "test_scaled = ss.transform(test_input)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "lr=LogisticRegression(C=20, max_iter=1000)       # c = 10은 너무 과소적합, c = 50은 너무 과대적합\n",
        "                                                 # 규제 제어 C : 기본값은 1, 적을수록 규제가 큼, 줄이면서 정확도 확인\n",
        "\n",
        "lr.fit(train_scaled, train_target)\n",
        "\n",
        "print(lr.score(train_scaled, train_target))\n",
        "print(lr.score(test_scaled, test_target))\n",
        "\n",
        "print(test_target)\n",
        "print(lr.predict(test_scaled))\n",
        "\n",
        "print(lr.classes_)\n",
        "print(np.round(lr.predict_proba(test_scaled), decimals=1))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "fish = pd.read_csv('http://bit.ly/fish_csv_data')\n",
        "fish_input = fish[['Weight','Length','Diagonal','Height','Width']].to_numpy()\n",
        "fish_target = fish['Species'].to_numpy()\n",
        "\n",
        "train_input, test_input, train_target, test_target = train_test_split(fish_input, fish_target, random_state=42)\n",
        "\n",
        "ss = StandardScaler()\n",
        "ss.fit(train_input)\n",
        "train_scaled = ss.transform(train_input)\n",
        "test_scaled = ss.transform(test_input)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "lr=LogisticRegression(C=20, max_iter=1000)\n",
        "lr.fit(train_scaled, train_target)\n",
        "\n",
        "#print(lr.score(train_scaled,train_target))\n",
        "#print(lr.score(test_scaled, test_target))\n",
        "\n",
        "print(test_target[:5])\n",
        "print(lr.predict(test_scaled[:5]))\n",
        "print(lr.classes_)\n",
        "print(np.round(lr.predict_proba(test_scaled[:5]), decimals=3))\n",
        "\n",
        "print(lr.coef_.shape, lr.intercept_.shape)\n",
        "print(lr.coef_, lr.intercept_)"
      ],
      "metadata": {
        "id": "pYMieCA7ekBZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}